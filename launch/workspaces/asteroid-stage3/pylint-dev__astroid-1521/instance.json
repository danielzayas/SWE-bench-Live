{
  "all_hints_text": "`LookupMixIn` uses `lru_cache` on its `lookup` method with unlimited size, that means any object of the classes that uses `LookupMixIn` will be cached indefinitely.\nIs there any workaround for this? \nperhaps this: https://stackoverflow.com/a/50699209\r\n```py\r\nimport functools\r\nimport gc\r\n\r\ngc.collect()\r\nwrappers = [\r\n    a for a in gc.get_objects() \r\n    if isinstance(a, functools._lru_cache_wrapper)]\r\n\r\nfor wrapper in wrappers:\r\n    wrapper.cache_clear()\r\n```\nI wonder if it would make sense for astroid to implement public APIs for controlling caches? For example, astroid could have its own `lru_cache` wrapper which would record all created LRU caches in a weak container and allow to clear all of them. @Pierre-Sassoulas wdyt?\r\n\r\nAs an aside, pylint has similar issues and any solution we implement here could probably be applied to pylint as well.\nRight now the \"cache\" is simply a mutable default argument value in a function, so some change needs to be made to makes that happen. What do you have in mind for this API @superbobry  ?\nSorry, if I was vague. I was thinking of something like\r\n\r\n```python\r\nLRU_CACHES = weakref.WeakSet()\r\n\r\ndef lru_cache(maxsize=128, typed=False):\r\n  def wrapper(func):\r\n    cached_func = functools.lru_cache(maxsize=maxsize, typed=typed)(func)\r\n    LRU_CACHES.add(cached_func)\r\n    return cached_func\r\n  return wrapper\r\n\r\n# ...\r\n\r\ndef clear_caches():\r\n  for c in LRU_CACHES:\r\n    c.cache_clear()\r\n```\r\n    \r\nso that any internal astroid/pylint API which needs to be cached is explicitly tracked and invalidated via `clear_caches()`. I suspect we might also consider invaliding `MANAGER` caches, but I'm not very familiar with what it caches&when. \nThis seem better than what we have right now. But I think we need some auto-clearing mechanism because what would be the trigger to clear the cache in pylint otherwise ? I'm absolutely not a caching expert, but maybe we can count the number of time a cache is used and keep only the \"useful\" entry?\nI was assuming `clear_caches` is a public endpoint which can be called by astroid/pylint users. Wdyt?\nLast time I check the problem is that the cache (or is it a variable) is a tree of module dependencies. That means for example the os module could be cached twice if we are linting two different files which import the os module. It would be more efficient both in memory and execution to use a key based cache.\nHey @char101, this sound interesting. Could you link me to the relevant bits of the code where this is happening?\nSorry it was more than a year ago. Looking at pylint memory usage now seems to be much better.\r\n\r\n**module.py**\r\n```python\r\nimport os\r\n```\r\n\r\n**main.py**\r\n```python\r\nimport gc\r\n\r\nimport astroid\r\nimport psutil\r\n\r\nfrom pylint import lint\r\nfrom pylint.reporters.base_reporter import BaseReporter\r\n\r\n\r\nclass Reporter(BaseReporter):\r\n    def _display(self, layout):\r\n        pass\r\n\r\n\r\ndef main():\r\n    prev = psutil.Process().memory_info().rss\r\n    print(prev)\r\n    for _ in range(0, 30):\r\n        lint.Run(['--reports', 'n', 'module.py'], reporter=Reporter(), exit=False)\r\n        gc.collect()\r\n        rss = psutil.Process().memory_info().rss\r\n        print(rss, rss - prev)\r\n        prev = rss\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```\r\n\r\n```\r\n40730624\r\n48734208 8003584\r\n48771072 36864\r\n48779264 8192\r\n48783360 4096\r\n48807936 24576\r\n48820224 12288\r\n48840704 20480\r\n48922624 81920\r\n48955392 32768\r\n48959488 4096\r\n48959488 0\r\n48967680 8192\r\n48988160 20480\r\n49008640 20480\r\n49020928 12288\r\n49090560 69632\r\n49090560 0\r\n49090560 0\r\n49090560 0\r\n49102848 12288\r\n49119232 16384\r\n49143808 24576\r\n49221632 77824\r\n49221632 0\r\n49221632 0\r\n49221632 0\r\n49221632 0\r\n49287168 65536\r\n49287168 0\r\n49287168 0\r\n```\r\n\r\nUnfortunately recent pylint installed from pypi would throw an exception when linting pyparsing.py\r\n\r\n```\r\nException on node <ClassDef.ParseResults l.502 at 0x2052926eca0> in file 'R:\\pylint\\pyparsing1.py'\r\nTraceback (most recent call last):\r\n  File \"R:\\pylint\\main.py\", line 26, in <module>\r\n    main()\r\n  File \"R:\\pylint\\main.py\", line 18, in main\r\n    lint.Run(['--reports', 'n', 'pyparsing1.py'], reporter=Reporter(), exit=False)\r\n  File \"R:\\pylint\\env\\lib\\site-packages\\pylint\\lint\\run.py\", line 384, in __init__\r\n    linter.check(args)\r\n  File \"R:\\pylint\\env\\lib\\site-packages\\pylint\\lint\\pylinter.py\", line 975, in check\r\n    self._check_files(\r\n  File \"R:\\pylint\\env\\lib\\site-packages\\pylint\\lint\\pylinter.py\", line 1009, in _check_files\r\n    self._check_file(get_ast, check_astroid_module, name, filepath, modname)\r\n  File \"R:\\pylint\\env\\lib\\site-packages\\pylint\\lint\\pylinter.py\", line 1035, in _check_file\r\n    check_astroid_module(ast_node)\r\n  File \"R:\\pylint\\env\\lib\\site-packages\\pylint\\lint\\pylinter.py\", line 1172, in check_astroid_module\r\n    retval = self._check_astroid_module(\r\n  File \"R:\\pylint\\env\\lib\\site-packages\\pylint\\lint\\pylinter.py\", line 1217, in _check_astroid_module\r\n    walker.walk(ast_node)\r\n  File \"R:\\pylint\\env\\lib\\site-packages\\pylint\\utils\\ast_walker.py\", line 77, in walk\r\n    self.walk(child)\r\n  File \"R:\\pylint\\env\\lib\\site-packages\\pylint\\utils\\ast_walker.py\", line 79, in walk\r\n    callback(astroid)\r\n  File \"R:\\pylint\\env\\lib\\site-packages\\pylint\\checkers\\classes.py\", line 904, in leave_classdef\r\n    self._check_unused_private_attributes(node)\r\n  File \"R:\\pylint\\env\\lib\\site-packages\\pylint\\checkers\\classes.py\", line 983, in _check_unused_private_attributes\r\n    assign_attr.expr.name == \"cls\"\r\nAttributeError: 'Subscript' object has no attribute 'name'\r\n```\r\nWith 245 KB test_tables.py from pytables\r\n\r\n```\r\n40759296\r\n186531840 145772544\r\n207368192 20836352\r\n223911936 16543744\r\n223764480 -147456\r\n225419264 1654784\r\n225538048 118784\r\n225759232 221184\r\n225861632 102400\r\n226070528 208896\r\n226131968 61440\r\n226328576 196608\r\n226582528 253952\r\n226619392 36864\r\n226947072 327680\r\n227172352 225280\r\n227127296 -45056\r\n227127296 0\r\n227164160 36864\r\n227405824 241664\r\n227405824 0\r\n227418112 12288\r\n227495936 77824\r\n227700736 204800\r\n227700736 0\r\n227708928 8192\r\n227840000 131072\r\n227971072 131072\r\n227971072 0\r\n227975168 4096\r\n228040704 65536\r\n```\n@char101 are you using the latest pylint ? This is supposed to be fixed by https://github.com/PyCQA/pylint/issues/4439 (ie. astroid 2.6.5)\nI was using the version in pypi\r\n\r\n```\r\n> env\\scripts\\pip list\r\nPackage           Version\r\n----------------- -------\r\nastroid           2.6.6\r\ncolorama          0.4.4\r\nisort             5.9.3\r\nlazy-object-proxy 1.6.0\r\nmccabe            0.6.1\r\npip               20.2.3\r\npsutil            5.8.0\r\npylint            2.9.6\r\nsetuptools        49.2.1\r\ntoml              0.10.2\r\nwrapt             1.12.1\r\n```\nI've digged into this issue as it is still present in the latest release (2.8.4). The following tweaks combined fixed the memory leak for me (RSS is <100MB after parsing ~100K files ):\r\n\r\n1. Disabling `@lru_cache` used without capacity in the following places:\r\n    - `astroid/interpreter/_import/spec.py`\r\n    - `astroid/interpreter/objectmodel.py`\r\n    - `astroid/nodes/node_classes.py`\r\n    - `astroid/transforms.py`\r\n2. Disabling `@_cached_generator` defined and used in `inference.py`\r\n3. Calling `astroid.astroid_manager.MANAGER.clear_cache()`\r\n4. Calling `astroid.context.clear_inference_tip_cache()`\r\n\r\nIn summary, I think we need to implement a common custom caching mechanism as suggested by @superbobry, and expose  an API to clear all the caches. What do you think?\nThank you for digging into this @keichi, maybe we can handle 1) by specifying the capacity in `@lru_cache` and adding an API to change it programmatically ? For 2) I don't know the implication of removing it, seems like it's all or nothing ? For 3/ and 4/ we can add an api to clear all caches instead of having to call multiple function ? Is this what you had in mind ?\nAnother option is to use sqlite or a key value database to store the cache (possibly compressed), then memory wouldn't be a problem. It might also help speed up execution of the cli command.\n@char101switching to SQLite would hide the problem instead, but it wouldn't fix it. Any unbounded cache indexed on an astroid AST node is leaky when pylint is used as a library. On top of that, bounded caches could also be expensive if they retain a reference to the linter (indirectly ofc).\r\n\r\n@keichi are you willing to give the common caching mechanism a go? Happy to review the PR if @Pierre-Sassoulas agrees it is worth exploring.\r\n\r\n@Pierre-Sassoulas my suggestion was to add an API for flushing all caches. I don't have a use-case for changing cache capacity on the fly, but perhaps others do? I would say, thought that all caches should be bounded and even with that it might still be desirable to flush them periodically/in between API calls to reduce memory usage.\n> Happy to review the PR if @Pierre-Sassoulas agrees it is worth exploring.\r\n\r\nYes, continuously increasing cache without any solution to clear it is bad :) \r\n\r\n> I don't have a use-case for changing cache capacity on the fly\r\n\r\nNot on the fly, but I guess astroid run on pretty various architectures. If it's hard coded the question of what bound should be applied exactly will rise. The default value we could use is probably in the range of 2g - 8g, but being able to change the value if you have 256g of RAM will arise at some point I assume.\nThe module ast references each other right? Trimming the cache might not necessarily frees the objects unless the whole cache is cleared.\r\n\r\nRemoving the module ast from the cache while it is still being referenced by other modules could result in memory leak.\r\n\r\nSo to be safe it might be necessary to loop the cache, use the gc module to find the objects where the referer count is only 1 and remove only those objects from the cache. And this process might need to be run several times until the target memory is reached.\nI'd be happy to draft a PR to introduce a new caching mechanism with help from you guys.\r\n\r\nI agree the capacity should be configurable but a flushing API is also necessary in certain use cases. For example, I am writing a tool that parses every file in a repository for every git commit, where I need to empty the cache when checking out a new commit.\r\n\r\nMy suggestion would be to introduce a cache with a user-configurable capacity and a function to flush all caches.\nAll the modules AST references each other in what become a large graph. To trim a graph we need to start from the leaf nodes so IMO the focus should not be on the cache mechanism only since the cache mechanism are only part of the graph.\r\n\r\nAlternatively the modules AST should refer to each othes using a weakref, and when the weakref is null, then trigger a module reparse.\r\n\r\nI think the cache is simply a key index (where the key = module name) to an in-memory graph database.\n@char101 Are you talking about `AstroidManager.astroid_cache`? I don't mean to touch it.\r\n\r\nThe problems here are `@lru_cache` and `@_cached_generator` because they create references from the global namaespace that cannot be removed in the current design. Even `AstroidManager.clear_cache()` will not remove these references.\r\n\r\nAny circular references between the graph nodes are fine because the nodes will be eventually collected by the GC once they become unreachable. `@lru_cache` and `@_cached_generator` create references from the global namespace and thus a problem.\r\n\nI've found another suspicious bit:\r\n\r\nhttps://github.com/PyCQA/astroid/blob/47e860f7d5c73d53de77e7c450535e709e5ef99e/astroid/interpreter/objectmodel.py#L95-L97\r\n\r\nThis mutates `self`. However, some object models seem to be class-level descriptors, e.g.\r\n\r\nhttps://github.com/PyCQA/astroid/blob/47e860f7d5c73d53de77e7c450535e709e5ef99e/astroid/nodes/scoped_nodes.py#L2135-L2159\r\n\r\nI was able to reproduce a leak even with all caches explicitly flushed by linting just `class A: ...`.\r\n\r\n@keichi as a slight aside, maybe a better way to solve the caching issue once and for all is to introduce a concept of session and only allow caching things in the session object?\nChanging `ObjectModel.__call__` to return a new instance helps, but there seem to still be other leaks. So, even after doing this and all of the steps outlined by @keichi above, the memory still goes up (and you could see the number of AST nodes in `gc.get_objects()` ~doubling at every lint call).\nHmm strange, I knew there was a small leak even after clearing the cache but it was small enough that it was practically not a problem (I'm able to parse 100K files as I wrote earlier).\r\n\r\nThe session approach would be ideal in the long term. It would involve a lot of refactoring and breaking changes to the API though. Maybe we can base it off of `AstroidManager`?\n\n",
  "base_commit": "f125fa7a5c95b59c39c8ca6a0276c51cef55c035",
  "commit_urls": [
    "https://github.com/pylint-dev/astroid/commit/35171c446a8fa4cfccd7cf6a96a841c3afee4bb8",
    "https://github.com/pylint-dev/astroid/commit/0ea952c5069dc1fa7290c975269b1ecbf3f525fc",
    "https://github.com/pylint-dev/astroid/commit/f70ef40c4c286b9e63a7e1c1111dbfbf7f71da6d",
    "https://github.com/pylint-dev/astroid/commit/035902579a07222f6ea121e442e039516ff117d3",
    "https://github.com/pylint-dev/astroid/commit/0d4c0d3323b15a38ab663956ef2eb8d7978f7ec7",
    "https://github.com/pylint-dev/astroid/commit/37b1d91317aa8f7a6a92cf6d10e16c6d7382707a",
    "https://github.com/pylint-dev/astroid/commit/5fd833afeb0be814d025e71a59650f63bf183388",
    "https://github.com/pylint-dev/astroid/commit/c669f8d8598baef61db16c1072e497b6bac10941",
    "https://github.com/pylint-dev/astroid/commit/b5034cdba5717ff767435ab11054e44d276ca91a",
    "https://github.com/pylint-dev/astroid/commit/e83701c1cf79b441247b8f94d3057b573ff11c3b",
    "https://github.com/pylint-dev/astroid/commit/012f32f40a77c89743bbbe22f23528b77e9f7b95",
    "https://github.com/pylint-dev/astroid/commit/7bbc7025afae2b424e1aa7e9627a4bdfb015896c",
    "https://github.com/pylint-dev/astroid/commit/8ee0632af1786b9646db83a98c87d3232d92e7b9",
    "https://github.com/pylint-dev/astroid/commit/46d82356dde18fc5dea600530b60e7a32d876604",
    "https://github.com/pylint-dev/astroid/commit/b8ac347d0258296965b2c6607697e81cd992fa6d",
    "https://github.com/pylint-dev/astroid/commit/a88b481e5614e8cf110ea1e51d7236b917ac4173",
    "https://github.com/pylint-dev/astroid/commit/4427b71cba2539916f0e637fda4564978338055c",
    "https://github.com/pylint-dev/astroid/commit/6f98e3027dc27d11249d2fc4b11179c99bba796e",
    "https://github.com/pylint-dev/astroid/commit/e0797d4b4256809252f4b9f127e3dab1ef1267f7",
    "https://github.com/pylint-dev/astroid/commit/ea74ae49cdadcf08520baaf1bd727a246a294fda"
  ],
  "created_at": "2022-04-20T13:33:00Z",
  "hints_text": "`LookupMixIn` uses `lru_cache` on its `lookup` method with unlimited size, that means any object of the classes that uses `LookupMixIn` will be cached indefinitely.\nIs there any workaround for this? \nperhaps this: https://stackoverflow.com/a/50699209\r\n```py\r\nimport functools\r\nimport gc\r\n\r\ngc.collect()\r\nwrappers = [\r\n    a for a in gc.get_objects() \r\n    if isinstance(a, functools._lru_cache_wrapper)]\r\n\r\nfor wrapper in wrappers:\r\n    wrapper.cache_clear()\r\n```\nI wonder if it would make sense for astroid to implement public APIs for controlling caches? For example, astroid could have its own `lru_cache` wrapper which would record all created LRU caches in a weak container and allow to clear all of them. @Pierre-Sassoulas wdyt?\r\n\r\nAs an aside, pylint has similar issues and any solution we implement here could probably be applied to pylint as well.\nRight now the \"cache\" is simply a mutable default argument value in a function, so some change needs to be made to makes that happen. What do you have in mind for this API @superbobry  ?\nSorry, if I was vague. I was thinking of something like\r\n\r\n```python\r\nLRU_CACHES = weakref.WeakSet()\r\n\r\ndef lru_cache(maxsize=128, typed=False):\r\n  def wrapper(func):\r\n    cached_func = functools.lru_cache(maxsize=maxsize, typed=typed)(func)\r\n    LRU_CACHES.add(cached_func)\r\n    return cached_func\r\n  return wrapper\r\n\r\n# ...\r\n\r\ndef clear_caches():\r\n  for c in LRU_CACHES:\r\n    c.cache_clear()\r\n```\r\n    \r\nso that any internal astroid/pylint API which needs to be cached is explicitly tracked and invalidated via `clear_caches()`. I suspect we might also consider invaliding `MANAGER` caches, but I'm not very familiar with what it caches&when. \nThis seem better than what we have right now. But I think we need some auto-clearing mechanism because what would be the trigger to clear the cache in pylint otherwise ? I'm absolutely not a caching expert, but maybe we can count the number of time a cache is used and keep only the \"useful\" entry?\nI was assuming `clear_caches` is a public endpoint which can be called by astroid/pylint users. Wdyt?\nLast time I check the problem is that the cache (or is it a variable) is a tree of module dependencies. That means for example the os module could be cached twice if we are linting two different files which import the os module. It would be more efficient both in memory and execution to use a key based cache.\nHey @char101, this sound interesting. Could you link me to the relevant bits of the code where this is happening?\nSorry it was more than a year ago. Looking at pylint memory usage now seems to be much better.\r\n\r\n**module.py**\r\n```python\r\nimport os\r\n```\r\n\r\n**main.py**\r\n```python\r\nimport gc\r\n\r\nimport astroid\r\nimport psutil\r\n\r\nfrom pylint import lint\r\nfrom pylint.reporters.base_reporter import BaseReporter\r\n\r\n\r\nclass Reporter(BaseReporter):\r\n    def _display(self, layout):\r\n        pass\r\n\r\n\r\ndef main():\r\n    prev = psutil.Process().memory_info().rss\r\n    print(prev)\r\n    for _ in range(0, 30):\r\n        lint.Run(['--reports', 'n', 'module.py'], reporter=Reporter(), exit=False)\r\n        gc.collect()\r\n        rss = psutil.Process().memory_info().rss\r\n        print(rss, rss - prev)\r\n        prev = rss\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```\r\n\r\n```\r\n40730624\r\n48734208 8003584\r\n48771072 36864\r\n48779264 8192\r\n48783360 4096\r\n48807936 24576\r\n48820224 12288\r\n48840704 20480\r\n48922624 81920\r\n48955392 32768\r\n48959488 4096\r\n48959488 0\r\n48967680 8192\r\n48988160 20480\r\n49008640 20480\r\n49020928 12288\r\n49090560 69632\r\n49090560 0\r\n49090560 0\r\n49090560 0\r\n49102848 12288\r\n49119232 16384\r\n49143808 24576\r\n49221632 77824\r\n49221632 0\r\n49221632 0\r\n49221632 0\r\n49221632 0\r\n49287168 65536\r\n49287168 0\r\n49287168 0\r\n```\r\n\r\nUnfortunately recent pylint installed from pypi would throw an exception when linting pyparsing.py\r\n\r\n```\r\nException on node <ClassDef.ParseResults l.502 at 0x2052926eca0> in file 'R:\\pylint\\pyparsing1.py'\r\nTraceback (most recent call last):\r\n  File \"R:\\pylint\\main.py\", line 26, in <module>\r\n    main()\r\n  File \"R:\\pylint\\main.py\", line 18, in main\r\n    lint.Run(['--reports', 'n', 'pyparsing1.py'], reporter=Reporter(), exit=False)\r\n  File \"R:\\pylint\\env\\lib\\site-packages\\pylint\\lint\\run.py\", line 384, in __init__\r\n    linter.check(args)\r\n  File \"R:\\pylint\\env\\lib\\site-packages\\pylint\\lint\\pylinter.py\", line 975, in check\r\n    self._check_files(\r\n  File \"R:\\pylint\\env\\lib\\site-packages\\pylint\\lint\\pylinter.py\", line 1009, in _check_files\r\n    self._check_file(get_ast, check_astroid_module, name, filepath, modname)\r\n  File \"R:\\pylint\\env\\lib\\site-packages\\pylint\\lint\\pylinter.py\", line 1035, in _check_file\r\n    check_astroid_module(ast_node)\r\n  File \"R:\\pylint\\env\\lib\\site-packages\\pylint\\lint\\pylinter.py\", line 1172, in check_astroid_module\r\n    retval = self._check_astroid_module(\r\n  File \"R:\\pylint\\env\\lib\\site-packages\\pylint\\lint\\pylinter.py\", line 1217, in _check_astroid_module\r\n    walker.walk(ast_node)\r\n  File \"R:\\pylint\\env\\lib\\site-packages\\pylint\\utils\\ast_walker.py\", line 77, in walk\r\n    self.walk(child)\r\n  File \"R:\\pylint\\env\\lib\\site-packages\\pylint\\utils\\ast_walker.py\", line 79, in walk\r\n    callback(astroid)\r\n  File \"R:\\pylint\\env\\lib\\site-packages\\pylint\\checkers\\classes.py\", line 904, in leave_classdef\r\n    self._check_unused_private_attributes(node)\r\n  File \"R:\\pylint\\env\\lib\\site-packages\\pylint\\checkers\\classes.py\", line 983, in _check_unused_private_attributes\r\n    assign_attr.expr.name == \"cls\"\r\nAttributeError: 'Subscript' object has no attribute 'name'\r\n```\r\nWith 245 KB test_tables.py from pytables\r\n\r\n```\r\n40759296\r\n186531840 145772544\r\n207368192 20836352\r\n223911936 16543744\r\n223764480 -147456\r\n225419264 1654784\r\n225538048 118784\r\n225759232 221184\r\n225861632 102400\r\n226070528 208896\r\n226131968 61440\r\n226328576 196608\r\n226582528 253952\r\n226619392 36864\r\n226947072 327680\r\n227172352 225280\r\n227127296 -45056\r\n227127296 0\r\n227164160 36864\r\n227405824 241664\r\n227405824 0\r\n227418112 12288\r\n227495936 77824\r\n227700736 204800\r\n227700736 0\r\n227708928 8192\r\n227840000 131072\r\n227971072 131072\r\n227971072 0\r\n227975168 4096\r\n228040704 65536\r\n```\n@char101 are you using the latest pylint ? This is supposed to be fixed by https://github.com/PyCQA/pylint/issues/4439 (ie. astroid 2.6.5)\nI was using the version in pypi\r\n\r\n```\r\n> env\\scripts\\pip list\r\nPackage           Version\r\n----------------- -------\r\nastroid           2.6.6\r\ncolorama          0.4.4\r\nisort             5.9.3\r\nlazy-object-proxy 1.6.0\r\nmccabe            0.6.1\r\npip               20.2.3\r\npsutil            5.8.0\r\npylint            2.9.6\r\nsetuptools        49.2.1\r\ntoml              0.10.2\r\nwrapt             1.12.1\r\n```\nI've digged into this issue as it is still present in the latest release (2.8.4). The following tweaks combined fixed the memory leak for me (RSS is <100MB after parsing ~100K files ):\r\n\r\n1. Disabling `@lru_cache` used without capacity in the following places:\r\n    - `astroid/interpreter/_import/spec.py`\r\n    - `astroid/interpreter/objectmodel.py`\r\n    - `astroid/nodes/node_classes.py`\r\n    - `astroid/transforms.py`\r\n2. Disabling `@_cached_generator` defined and used in `inference.py`\r\n3. Calling `astroid.astroid_manager.MANAGER.clear_cache()`\r\n4. Calling `astroid.context.clear_inference_tip_cache()`\r\n\r\nIn summary, I think we need to implement a common custom caching mechanism as suggested by @superbobry, and expose  an API to clear all the caches. What do you think?\nThank you for digging into this @keichi, maybe we can handle 1) by specifying the capacity in `@lru_cache` and adding an API to change it programmatically ? For 2) I don't know the implication of removing it, seems like it's all or nothing ? For 3/ and 4/ we can add an api to clear all caches instead of having to call multiple function ? Is this what you had in mind ?\nAnother option is to use sqlite or a key value database to store the cache (possibly compressed), then memory wouldn't be a problem. It might also help speed up execution of the cli command.\n@char101switching to SQLite would hide the problem instead, but it wouldn't fix it. Any unbounded cache indexed on an astroid AST node is leaky when pylint is used as a library. On top of that, bounded caches could also be expensive if they retain a reference to the linter (indirectly ofc).\r\n\r\n@keichi are you willing to give the common caching mechanism a go? Happy to review the PR if @Pierre-Sassoulas agrees it is worth exploring.\r\n\r\n@Pierre-Sassoulas my suggestion was to add an API for flushing all caches. I don't have a use-case for changing cache capacity on the fly, but perhaps others do? I would say, thought that all caches should be bounded and even with that it might still be desirable to flush them periodically/in between API calls to reduce memory usage.\n> Happy to review the PR if @Pierre-Sassoulas agrees it is worth exploring.\r\n\r\nYes, continuously increasing cache without any solution to clear it is bad :) \r\n\r\n> I don't have a use-case for changing cache capacity on the fly\r\n\r\nNot on the fly, but I guess astroid run on pretty various architectures. If it's hard coded the question of what bound should be applied exactly will rise. The default value we could use is probably in the range of 2g - 8g, but being able to change the value if you have 256g of RAM will arise at some point I assume.\nThe module ast references each other right? Trimming the cache might not necessarily frees the objects unless the whole cache is cleared.\r\n\r\nRemoving the module ast from the cache while it is still being referenced by other modules could result in memory leak.\r\n\r\nSo to be safe it might be necessary to loop the cache, use the gc module to find the objects where the referer count is only 1 and remove only those objects from the cache. And this process might need to be run several times until the target memory is reached.\nI'd be happy to draft a PR to introduce a new caching mechanism with help from you guys.\r\n\r\nI agree the capacity should be configurable but a flushing API is also necessary in certain use cases. For example, I am writing a tool that parses every file in a repository for every git commit, where I need to empty the cache when checking out a new commit.\r\n\r\nMy suggestion would be to introduce a cache with a user-configurable capacity and a function to flush all caches.\nAll the modules AST references each other in what become a large graph. To trim a graph we need to start from the leaf nodes so IMO the focus should not be on the cache mechanism only since the cache mechanism are only part of the graph.\r\n\r\nAlternatively the modules AST should refer to each othes using a weakref, and when the weakref is null, then trigger a module reparse.\r\n\r\nI think the cache is simply a key index (where the key = module name) to an in-memory graph database.\n@char101 Are you talking about `AstroidManager.astroid_cache`? I don't mean to touch it.\r\n\r\nThe problems here are `@lru_cache` and `@_cached_generator` because they create references from the global namaespace that cannot be removed in the current design. Even `AstroidManager.clear_cache()` will not remove these references.\r\n\r\nAny circular references between the graph nodes are fine because the nodes will be eventually collected by the GC once they become unreachable. `@lru_cache` and `@_cached_generator` create references from the global namespace and thus a problem.\r\n\nI've found another suspicious bit:\r\n\r\nhttps://github.com/PyCQA/astroid/blob/47e860f7d5c73d53de77e7c450535e709e5ef99e/astroid/interpreter/objectmodel.py#L95-L97\r\n\r\nThis mutates `self`. However, some object models seem to be class-level descriptors, e.g.\r\n\r\nhttps://github.com/PyCQA/astroid/blob/47e860f7d5c73d53de77e7c450535e709e5ef99e/astroid/nodes/scoped_nodes.py#L2135-L2159\r\n\r\nI was able to reproduce a leak even with all caches explicitly flushed by linting just `class A: ...`.\r\n\r\n@keichi as a slight aside, maybe a better way to solve the caching issue once and for all is to introduce a concept of session and only allow caching things in the session object?\nChanging `ObjectModel.__call__` to return a new instance helps, but there seem to still be other leaks. So, even after doing this and all of the steps outlined by @keichi above, the memory still goes up (and you could see the number of AST nodes in `gc.get_objects()` ~doubling at every lint call).\nHmm strange, I knew there was a small leak even after clearing the cache but it was small enough that it was practically not a problem (I'm able to parse 100K files as I wrote earlier).\r\n\r\nThe session approach would be ideal in the long term. It would involve a lot of refactoring and breaking changes to the API though. Maybe we can base it off of `AstroidManager`?\n\n",
  "instance_id": "pylint-dev__astroid-1521",
  "issue_numbers": [
    792
  ],
  "language": "python",
  "patch": "diff --git a/astroid/inference.py b/astroid/inference.py\nindex cd66a91709..8d85664b34 100644\n--- a/astroid/inference.py\n+++ b/astroid/inference.py\n@@ -23,8 +23,6 @@\n     Union,\n )\n \n-import wrapt\n-\n from astroid import bases, decorators, helpers, nodes, protocols, util\n from astroid.context import (\n     CallContext,\n@@ -1037,28 +1035,6 @@ def infer_ifexp(self, context=None):\n nodes.IfExp._infer = infer_ifexp  # type: ignore[assignment]\n \n \n-# pylint: disable=dangerous-default-value\n-@wrapt.decorator\n-def _cached_generator(\n-    func, instance: _FunctionDefT, args, kwargs, _cache={}  # noqa: B006\n-):\n-    node = instance\n-    try:\n-        return iter(_cache[func, id(node)])\n-    except KeyError:\n-        result = func(*args, **kwargs)\n-        # Need to keep an iterator around\n-        original, copy = itertools.tee(result)\n-        _cache[func, id(node)] = list(copy)\n-        return original\n-\n-\n-# When inferring a property, we instantiate a new `objects.Property` object,\n-# which in turn, because it inherits from `FunctionDef`, sets itself in the locals\n-# of the wrapping frame. This means that every time we infer a property, the locals\n-# are mutated with a new instance of the property. This is why we cache the result\n-# of the function's inference.\n-@_cached_generator\n def infer_functiondef(\n     self: _FunctionDefT, context: Optional[InferenceContext] = None\n ) -> Generator[Union[\"Property\", _FunctionDefT], None, InferenceErrorInfo]:\n@@ -1066,13 +1042,25 @@ def infer_functiondef(\n         yield self\n         return InferenceErrorInfo(node=self, context=context)\n \n+    # When inferring a property, we instantiate a new `objects.Property` object,\n+    # which in turn, because it inherits from `FunctionDef`, sets itself in the locals\n+    # of the wrapping frame. This means that every time we infer a property, the locals\n+    # are mutated with a new instance of the property. To avoid this, we detect this\n+    # scenario and avoid passing the `parent` argument to the constructor.\n+    parent_frame = self.parent.frame(future=True)\n+    property_already_in_parent_locals = self.name in parent_frame.locals and any(\n+        isinstance(val, objects.Property) for val in parent_frame.locals[self.name]\n+    )\n+\n     prop_func = objects.Property(\n         function=self,\n         name=self.name,\n         lineno=self.lineno,\n-        parent=self.parent,\n+        parent=self.parent if not property_already_in_parent_locals else None,\n         col_offset=self.col_offset,\n     )\n+    if property_already_in_parent_locals:\n+        prop_func.parent = self.parent\n     prop_func.postinit(body=[], args=self.args, doc_node=self.doc_node)\n     yield prop_func\n     return InferenceErrorInfo(node=self, context=context)\ndiff --git a/astroid/interpreter/objectmodel.py b/astroid/interpreter/objectmodel.py\nindex cf9227b510..4fd79596b7 100644\n--- a/astroid/interpreter/objectmodel.py\n+++ b/astroid/interpreter/objectmodel.py\n@@ -25,6 +25,7 @@\n import os\n import pprint\n import types\n+from functools import lru_cache\n from typing import TYPE_CHECKING, List, Optional\n \n import astroid\n@@ -100,6 +101,7 @@ def __get__(self, instance, cls=None):\n     def __contains__(self, name):\n         return name in self.attributes()\n \n+    @lru_cache()  # noqa\n     def attributes(self) -> List[str]:\n         \"\"\"Get the attributes which are exported by this object model.\"\"\"\n         return [o[LEN_OF_IMPL_PREFIX:] for o in dir(self) if o.startswith(IMPL_PREFIX)]\ndiff --git a/astroid/manager.py b/astroid/manager.py\nindex 9651bfada9..653c1390a2 100644\n--- a/astroid/manager.py\n+++ b/astroid/manager.py\n@@ -18,6 +18,7 @@\n from astroid.interpreter._import import spec\n from astroid.modutils import (\n     NoSourceFile,\n+    _cache_normalize_path_,\n     file_info_from_modpath,\n     get_source_file,\n     is_module_name_part_of_extension_package_whitelist,\n@@ -365,8 +366,24 @@ def bootstrap(self):\n     def clear_cache(self) -> None:\n         \"\"\"Clear the underlying cache, bootstrap the builtins module and\n         re-register transforms.\"\"\"\n+        # import here because of cyclic imports\n+        # pylint: disable=import-outside-toplevel\n+        from astroid.inference_tip import clear_inference_tip_cache\n+        from astroid.interpreter.objectmodel import ObjectModel\n+        from astroid.nodes.node_classes import LookupMixIn\n+\n+        clear_inference_tip_cache()\n+\n         self.astroid_cache.clear()\n         AstroidManager.brain[\"_transform\"] = TransformVisitor()\n+\n+        for lru_cache in (\n+            LookupMixIn.lookup,\n+            _cache_normalize_path_,\n+            ObjectModel.attributes,\n+        ):\n+            lru_cache.cache_clear()\n+\n         self.bootstrap()\n \n         # Reload brain plugins. During initialisation this is done in astroid.__init__.py\ndiff --git a/astroid/modutils.py b/astroid/modutils.py\nindex cfb3f85184..06fda983d7 100644\n--- a/astroid/modutils.py\n+++ b/astroid/modutils.py\n@@ -22,8 +22,9 @@\n import sys\n import sysconfig\n import types\n+from functools import lru_cache\n from pathlib import Path\n-from typing import Dict, Set\n+from typing import Set\n \n from astroid.const import IS_JYTHON, IS_PYPY\n from astroid.interpreter._import import spec, util\n@@ -138,7 +139,9 @@ def _handle_blacklist(blacklist, dirnames, filenames):\n             filenames.remove(norecurs)\n \n \n-_NORM_PATH_CACHE: Dict[str, str] = {}\n+@lru_cache()\n+def _cache_normalize_path_(path: str) -> str:\n+    return _normalize_path(path)\n \n \n def _cache_normalize_path(path: str) -> str:\n@@ -146,13 +149,9 @@ def _cache_normalize_path(path: str) -> str:\n     # _module_file calls abspath on every path in sys.path every time it's\n     # called; on a larger codebase this easily adds up to half a second just\n     # assembling path components. This cache alleviates that.\n-    try:\n-        return _NORM_PATH_CACHE[path]\n-    except KeyError:\n-        if not path:  # don't cache result for ''\n-            return _normalize_path(path)\n-        result = _NORM_PATH_CACHE[path] = _normalize_path(path)\n-        return result\n+    if not path:  # don't cache result for ''\n+        return _normalize_path(path)\n+    return _cache_normalize_path_(path)\n \n \n def load_module_from_name(dotted_name: str) -> types.ModuleType:\ndiff --git a/astroid/nodes/node_classes.py b/astroid/nodes/node_classes.py\nindex 0538d4d169..362c8f6817 100644\n--- a/astroid/nodes/node_classes.py\n+++ b/astroid/nodes/node_classes.py\n@@ -366,7 +366,7 @@ def get_children(self):\n class LookupMixIn:\n     \"\"\"Mixin to look up a name in the right scope.\"\"\"\n \n-    @lru_cache(maxsize=None)  # pylint: disable=cache-max-size-none  # noqa\n+    @lru_cache()  # noqa\n     def lookup(self, name: str) -> typing.Tuple[str, typing.List[NodeNG]]:\n         \"\"\"Lookup where the given variable is assigned.\n \n",
  "problem_statement": "Continuously increasing memory usage when pylint is run via its API\n### Steps to reproduce\r\n\r\nWhen using pylint in-process to lint a file (in this case in vim), the memory usage of the editor is continually increasing. The increase is even higher when the cache is cleared. I believe that this is caused because the module is still referenced by `lru_cache` even though it has been deleted from the manager cache and each new parsing adds a new cache entry.\r\n\r\n```python\r\nimport gc\r\n\r\nimport astroid\r\nimport psutil\r\nfrom pylint import lint\r\nfrom pylint.reporters.base_reporter import BaseReporter\r\n\r\n\r\nclass Reporter(BaseReporter):\r\n    def _display(self, layout):\r\n        pass\r\n\r\n\r\ndef run1():\r\n    print('run without clearing cache')\r\n    for _ in range(0, 5):\r\n        lint.Run(['--reports', 'n', 'pyparsing1.py'], reporter=Reporter(), exit=False)\r\n        print(psutil.Process().memory_info().rss)\r\n\r\n\r\ndef run2():\r\n    print('run with cache clear')\r\n    for _ in range(0, 5):\r\n        lint.Run(['--reports', 'n', 'pyparsing1.py'], reporter=Reporter(), exit=False)\r\n        astroid.builder.MANAGER.astroid_cache.clear()\r\n        gc.collect()\r\n        print(psutil.Process().memory_info().rss)\r\n\r\n\r\ndef run3():\r\n    print('run with clearning only checked module cache')\r\n    for _ in range(0, 5):\r\n        lint.Run(['--reports', 'n', 'pyparsing1.py'], reporter=Reporter(), exit=False)\r\n        del astroid.builder.MANAGER.astroid_cache['pyparsing1']\r\n        gc.collect()\r\n        print(psutil.Process().memory_info().rss)\r\n\r\n\r\ndef main():\r\n    run1()\r\n    run2()\r\n    run3()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nThe linted file is `253KB` `pyparsing.py` renamed to `pyparsing1.py`.\r\n\r\n```\r\nrun without clearing cache\r\n118296576\r\n130756608\r\n143630336\r\n156053504\r\n168882176\r\nrun with cache clear\r\n181768192\r\n263176192\r\n345382912\r\n427147264\r\n507686912\r\nrun with clearning only checked module cache\r\n589205504\r\n615079936\r\n641482752\r\n665550848\r\n689803264\r\n```\r\n\r\n### Current behavior\r\nIncreasing memory usage\r\n\r\n### Expected behavior\r\nStable memory usage\r\n\r\n\r\n### ``python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"`` output\r\n\r\n`2.4.1`\r\n\n",
  "pull_number": 1521,
  "repo": "pylint-dev/astroid",
  "test_patch": "diff --git a/tests/unittest_manager.py b/tests/unittest_manager.py\nindex 6f0a7b16f4..96239233b3 100644\n--- a/tests/unittest_manager.py\n+++ b/tests/unittest_manager.py\n@@ -16,7 +16,9 @@\n from astroid import manager, test_utils\n from astroid.const import IS_JYTHON\n from astroid.exceptions import AstroidBuildingError, AstroidImportError\n+from astroid.modutils import is_standard_module\n from astroid.nodes import Const\n+from astroid.nodes.scoped_nodes import ClassDef\n \n from . import resources\n \n@@ -317,6 +319,43 @@ def test_borg(self) -> None:\n \n \n class ClearCacheTest(unittest.TestCase, resources.AstroidCacheSetupMixin):\n+    def test_clear_cache_clears_other_lru_caches(self) -> None:\n+        lrus = (\n+            astroid.nodes.node_classes.LookupMixIn.lookup,\n+            astroid.modutils._cache_normalize_path_,\n+            astroid.interpreter.objectmodel.ObjectModel.attributes,\n+        )\n+\n+        # Get a baseline for the size of the cache after simply calling bootstrap()\n+        baseline_cache_infos = [lru.cache_info() for lru in lrus]\n+\n+        # Generate some hits and misses\n+        ClassDef().lookup(\"garbage\")\n+        is_standard_module(\"unittest\", std_path=[\"garbage_path\"])\n+        astroid.interpreter.objectmodel.ObjectModel().attributes()\n+\n+        # Did the hits or misses actually happen?\n+        incremented_cache_infos = [lru.cache_info() for lru in lrus]\n+        for incremented_cache, baseline_cache in zip(\n+            incremented_cache_infos, baseline_cache_infos\n+        ):\n+            with self.subTest(incremented_cache=incremented_cache):\n+                self.assertGreater(\n+                    incremented_cache.hits + incremented_cache.misses,\n+                    baseline_cache.hits + baseline_cache.misses,\n+                )\n+\n+        astroid.MANAGER.clear_cache()  # also calls bootstrap()\n+\n+        # The cache sizes are now as low or lower than the original baseline\n+        cleared_cache_infos = [lru.cache_info() for lru in lrus]\n+        for cleared_cache, baseline_cache in zip(\n+            cleared_cache_infos, baseline_cache_infos\n+        ):\n+            with self.subTest(cleared_cache=cleared_cache):\n+                # less equal because the \"baseline\" might have had multiple calls to bootstrap()\n+                self.assertLessEqual(cleared_cache.currsize, baseline_cache.currsize)\n+\n     def test_brain_plugins_reloaded_after_clearing_cache(self) -> None:\n         astroid.MANAGER.clear_cache()\n         format_call = astroid.extract_node(\"''.format()\")\ndiff --git a/tests/unittest_modutils.py b/tests/unittest_modutils.py\nindex 3fb92a845b..3d15fb632c 100644\n--- a/tests/unittest_modutils.py\n+++ b/tests/unittest_modutils.py\n@@ -179,7 +179,7 @@ def test_load_packages_without_init(self) -> None:\n         https://github.com/PyCQA/astroid/issues/1327\n         \"\"\"\n         tmp_dir = Path(tempfile.gettempdir())\n-        self.addCleanup(os.chdir, os.curdir)\n+        self.addCleanup(os.chdir, os.getcwd())\n         os.chdir(tmp_dir)\n \n         self.addCleanup(shutil.rmtree, tmp_dir / \"src\")\n@@ -288,6 +288,8 @@ def test_custom_path(self) -> None:\n         self.assertTrue(\n             modutils.is_standard_module(\"data.module\", (os.path.abspath(datadir),))\n         )\n+        # \"\" will evaluate to cwd\n+        self.assertTrue(modutils.is_standard_module(\"data.module\", (\"\",)))\n \n     def test_failing_edge_cases(self) -> None:\n         # using a subpackage/submodule path as std_path argument\n",
  "commit_url": "https://github.com/pylint-dev/astroid/tree/f125fa7a5c95b59c39c8ca6a0276c51cef55c035"
}